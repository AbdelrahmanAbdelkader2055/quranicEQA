{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932674c4",
   "metadata": {
    "id": "932674c4"
   },
   "source": [
    "###  Data Preperation (Importing, Reformatting, Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WawwgyywqDzN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WawwgyywqDzN",
    "outputId": "6fd2ffdf-4489-4617-fb22-eb52f8b3fcfa"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1fbb11-4042-47ed-9c1a-4281dec9a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install sentencepiece \n",
    "#!pip install protobuf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7669448d",
   "metadata": {
    "id": "7669448d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "#We will use the JSON library instead of pandas to import our dataset\n",
    "#The reason we will import our dataset in this fashion is because we want to have a higher level of\n",
    "#Freedom and versitility when processing our elements and modifying them, and because they're JSON\n",
    "#Files with different data structure compared to csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2233cd",
   "metadata": {
    "id": "4c2233cd"
   },
   "outputs": [],
   "source": [
    "#This function is responsible of traversing through the entire data set\n",
    "#It will compile lists of passages, questions, and answers.\n",
    "#So we can easily use them.\n",
    "\n",
    "import json\n",
    "\n",
    "def json_to_dict_converter(path):\n",
    "    my_dict = list()\n",
    "\n",
    "    #Open the JSON file with UTF-8 encoding to handle non-ASCII characters, such as Arabic\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        #Read the file and convert it into a list of strings (each line as an element)\n",
    "        json_list = list(f)\n",
    "\n",
    "        #Iterate over each JSON string in the list\n",
    "        for json_str in json_list:\n",
    "            #Parse the JSON string into a Python dictionary\n",
    "            result = json.loads(json_str)\n",
    "            #Append the dictionary to the list\n",
    "            my_dict.append(result)\n",
    "\n",
    "    return my_dict\n",
    "\n",
    "def dict_element_extractor(dict_object, key):\n",
    "    elements_list = list()\n",
    "\n",
    "    #Iterate over each dictionary in the list\n",
    "    for row_number in range(len(dict_object)):\n",
    "        #Extract the value associated with the given key and append it to the list\n",
    "        elements_list.append(dict_object[row_number][key])\n",
    "\n",
    "    return elements_list\n",
    "\n",
    "def multi_answer_split(passages, questions, answers):\n",
    "    #Initialize empty lists to store reformatted passages, questions, and answers\n",
    "    passages_reformatted = list()\n",
    "    questions_reformatted = list()\n",
    "    answers_reformatted = list()\n",
    "\n",
    "    #Iterate over each set of answers\n",
    "    for row_index in range(len(answers)):\n",
    "        #Iterate over each individual answer in the current set of answers\n",
    "        for answer in answers[row_index]:\n",
    "            #Append the answer to the reformatted answers list\n",
    "            answers_reformatted.append(answer)\n",
    "            #Append the corresponding passage and question to their respective lists\n",
    "            passages_reformatted.append(passages[row_index])\n",
    "            questions_reformatted.append(questions[row_index])\n",
    "\n",
    "    #Return the reformatted passages, questions, and answers as separate lists\n",
    "    return passages_reformatted, questions_reformatted, answers_reformatted\n",
    "\n",
    "def extract_json_contents(path):\n",
    "    #Convert the JSON file into a list of dictionaries\n",
    "    source_dict = json_to_dict_converter(path)\n",
    "    #Extract the \"passage\", \"question\", and \"answers\" elements from each dictionary\n",
    "    passages = dict_element_extractor(source_dict, \"passage\")\n",
    "    questions = dict_element_extractor(source_dict, \"question\")\n",
    "    answers = dict_element_extractor(source_dict, \"answers\")\n",
    "    #Split multiple answers into individual rows and return them\n",
    "    return multi_answer_split(passages, questions, answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ae12ac",
   "metadata": {
    "id": "c7ae12ac"
   },
   "outputs": [],
   "source": [
    "#Importing, reformatting, and splitting the datasets into lists\n",
    "train_passages, train_questions, train_answers = extract_json_contents(\"datasets/qrcd_v1.1_train.jsonl\")\n",
    "val_passages, val_questions, val_answers = extract_json_contents(\"datasets/qrcd_v1.1_dev.jsonl\")\n",
    "#test_passages, test_questions, test_answers = extract_json_contents(\"datasets/qrcd_v1.1_test_gold.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3127efdc-b283-434f-bb4a-5402fbe84ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861\n",
      "861\n",
      "861\n"
     ]
    }
   ],
   "source": [
    "#At this point all lists should have the same length\n",
    "print(len(train_passages))\n",
    "print(len(train_questions))\n",
    "print(len(train_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c994b95-0704-4799-a71d-f33c7a656a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(val_passages))\n",
    "print(len(val_questions))\n",
    "print(len(val_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b29c79-0a83-4e46-ac75-3bc56d2b9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function normalizes arabic text by removing unncessary diacritics that would negatively affect the model's performance\n",
    "#We need to remove the ones that would not affect the meaning of the words.\n",
    "def normalize_arabic(text):\n",
    "    #Compiling our matches for characters removal: الضمة والفاتحة والكسرة والسكون والشد والمد\n",
    "    tashkeel_pattern = re.compile(r'[\\u064B-\\u0652]')\n",
    "    #Substitutes the said characters with an empty string\n",
    "    normalized_text = re.sub(tashkeel_pattern, '', text)\n",
    "    return normalized_text\n",
    "    \n",
    "#This function should remove all of the unneeded digits and punctuations from whatever text we send to it\n",
    "def remove_digits_and_punctuations(text):\n",
    "    text = re.sub(\"[0-9]\", \"\", text)\n",
    "    my_punct = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "                '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_',\n",
    "                '`', '{', '|', '}', '~', '»', '«', '“', '”']\n",
    "    punct_pattern = re.compile(\"[\" + re.escape(\"\".join(my_punct)) + \"]\")\n",
    "    return re.sub(punct_pattern, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9135154f-f543-457b-b7e3-9e732a000915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to add end indices to the answers in the text, so we have both the beginning and end positions of our extracted answers\n",
    "def add_end_indices(answers, contexts):\n",
    "\n",
    "    #Loop through each answer and its corresponding context\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        #Extract the exact answer text (golden text) from the answer\n",
    "        golden_text = answer['text']\n",
    "        \n",
    "        #Get the starting index of the answer in the context\n",
    "        start_index = answer['start_char']\n",
    "        #Calculate the initial ending index of the answer based on the starting index and the length of the answer text\n",
    "        end_index = start_index + len(golden_text)\n",
    "        \n",
    "        #Ideally, the calculated start and end indices should match the exact position of the golden text in the context\n",
    "        if context[start_index:end_index] == golden_text:\n",
    "            #If the substring from start to end matches the golden text, assign the end index\n",
    "            answer['end_char'] = end_index\n",
    "        else:\n",
    "            #If the substring does not match, try adjusting the indices within a small range to account for potential mismatches\n",
    "            found = False\n",
    "            #Adjust the indices by searching within a range of -5 to +5 characters around the initial start index\n",
    "            for offset in range(-5, 6):\n",
    "                new_start = start_index + offset\n",
    "                new_end = new_start + len(golden_text)\n",
    "                #Check if the substring from the adjusted start to end matches the golden text\n",
    "                if context[new_start:new_end] == golden_text:\n",
    "                    #If a match is found, update the start and end indices\n",
    "                    answer['start_char'] = new_start\n",
    "                    answer['end_char'] = new_end\n",
    "                    found = True\n",
    "                    break\n",
    "            #If the answer is still not found, search the entire context to ensure the answer is found\n",
    "            if not found:\n",
    "                #Find the exact position of the golden text within the context\n",
    "                start_index = context.find(golden_text)\n",
    "                if start_index != -1:\n",
    "                    #If found, update the start and end indices accordingly\n",
    "                    answer['start_char'] = start_index\n",
    "                    answer['end_char'] = start_index + len(golden_text)\n",
    "                else:\n",
    "                    #If the golden text is not found in the context, raise an error (should not happen if assumption is correct)\n",
    "                    raise ValueError(f\"Answer text '{golden_text}' not found in context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73abf422-53b9-4b93-861d-0006ef518070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function helps modifying the text of the answers. We can now use the above preprocessing functions without affecting the structure of\n",
    "#answers dictionaries.\n",
    "def modify_text_in_answers(answers_list, text_modification_fn):\n",
    "    #Initialize an empty list to store the modified dictionaries\n",
    "    modified_answers = []\n",
    "    \n",
    "    #Iterate over each dictionary in the input list\n",
    "    for answer in answers_list:\n",
    "        #Create a copy of the current dictionary to avoid modifying the original\n",
    "        new_answer = answer.copy()\n",
    "        \n",
    "        #Check if the 'text' key exists in the dictionary\n",
    "        if 'text' in new_answer:\n",
    "            #Apply the text modification function to the 'text' key\n",
    "            new_answer['text'] = text_modification_fn(new_answer['text'])\n",
    "        #Add the modified dictionary to the new list\n",
    "        modified_answers.append(new_answer)\n",
    "\n",
    "    #Return the new list of modified dictionaries\n",
    "    return modified_answers\n",
    "\n",
    "#This function applies all of the preprocessing to our datasets\n",
    "def preprocess_dataset(passages_list, questions_list, answers_list):\n",
    "    #Normalizing Arabic text in passages, questions, and answers lists\n",
    "    passages_list_prepro = [normalize_arabic(passage) for passage in passages_list]\n",
    "    questions_list_prepro = [normalize_arabic(question) for question in questions_list]\n",
    "    answers_list_prepro = modify_text_in_answers(answers_list, normalize_arabic)\n",
    "\n",
    "    #Removing digits and punctuations from passages, questions, and answers lists\n",
    "    passages_list_prepro = [remove_digits_and_punctuations(passage) for passage in passages_list_prepro]\n",
    "    questions_list_prepro = [remove_digits_and_punctuations(question) for question in questions_list_prepro]\n",
    "    answers_list_prepro = modify_text_in_answers(answers_list_prepro, remove_digits_and_punctuations)\n",
    "\n",
    "    #Adding end indices to answers based on the preprocessed passages\n",
    "    add_end_indices(answers_list_prepro, passages_list_prepro)\n",
    "    \n",
    "    #Return the preprocessed passages, questions, and answers lists\n",
    "    return passages_list_prepro, questions_list_prepro, answers_list_prepro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2e5474-f8ec-4843-a2bd-1cb226516094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying all of the preprocessing\n",
    "train_passages_prepro, train_questions_prepro, train_answers_prepro = preprocess_dataset(train_passages, train_questions, train_answers)\n",
    "val_passages_prepro, val_questions_prepro, val_answers_prepro = preprocess_dataset(val_passages, val_questions, val_answers)\n",
    "\n",
    "#Adding end_char postitions for the non-preprocessed datasets, we will need them to compare both the preprocessed and vanilla sets model performances\n",
    "add_end_indices(train_answers, train_passages)\n",
    "add_end_indices(val_answers, val_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb44ad89-a167-4bd1-aad4-65f9c1afb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function prints our dataset lists. head_size represents the first n items in each list\n",
    "def print_set(passages_list, questions_list, answers_list, head_size = 1):\n",
    "    for i in range(head_size):\n",
    "        print(\"Passage \", i,\": \", passages_list[i])\n",
    "        print(\" Question \", i,\": \", questions_list[i])\n",
    "        print(\"Answer \", i,\": \", answers_list[i])\n",
    "        print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d94b39-eb54-4956-9b82-092bedfc86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocessing:\n",
      "Passage  0 :  ومن الناس من يقول آمنا بالله وباليوم الآخر وما هم بمؤمنين. يخادعون الله والذين آمنوا وما يخدعون إلا أنفسهم وما يشعرون. في قلوبهم مرض فزادهم الله مرضا ولهم عذاب أليم بما كانوا يكذبون. وإذا قيل لهم لا تفسدوا في الأرض قالوا إنما نحن مصلحون. ألا إنهم هم المفسدون ولكن لا يشعرون. وإذا قيل لهم آمنوا كما آمن الناس قالوا أنؤمن كما آمن السفهاء ألا إنهم هم السفهاء ولكن لا يعلمون. وإذا لقوا الذين آمنوا قالوا آمنا وإذا خلوا إلى شياطينهم قالوا إنا معكم إنما نحن مستهزئون. الله يستهزئ بهم ويمدهم في طغيانهم يعمهون. أولئك الذين اشتروا الضلالة بالهدى فما ربحت تجارتهم وما كانوا مهتدين.\n",
      " Question  0 :  لماذا سيُحاسب ويُعذب الضال يوم القيامة ان كان \"\"من يضلل الله فما له من هاد\"\" كما ورد من قوله تعالى في آية 23 و آية 36 من سورة الزمر؟\n",
      "Answer  0 :  {'text': 'أولئك الذين اشتروا الضلالة بالهدى', 'start_char': 504, 'end_char': 537}\n",
      "----------------------------------------------------\n",
      "After Preprocessing:\n",
      "Passage  0 :  ومن الناس من يقول آمنا بالله وباليوم الآخر وما هم بمؤمنين. يخادعون الله والذين آمنوا وما يخدعون إلا أنفسهم وما يشعرون. في قلوبهم مرض فزادهم الله مرضا ولهم عذاب أليم بما كانوا يكذبون. وإذا قيل لهم لا تفسدوا في الأرض قالوا إنما نحن مصلحون. ألا إنهم هم المفسدون ولكن لا يشعرون. وإذا قيل لهم آمنوا كما آمن الناس قالوا أنؤمن كما آمن السفهاء ألا إنهم هم السفهاء ولكن لا يعلمون. وإذا لقوا الذين آمنوا قالوا آمنا وإذا خلوا إلى شياطينهم قالوا إنا معكم إنما نحن مستهزئون. الله يستهزئ بهم ويمدهم في طغيانهم يعمهون. أولئك الذين اشتروا الضلالة بالهدى فما ربحت تجارتهم وما كانوا مهتدين.\n",
      " Question  0 :  لماذا سيحاسب ويعذب الضال يوم القيامة ان كان من يضلل الله فما له من هاد كما ورد من قوله تعالى في آية  و آية  من سورة الزمر؟\n",
      "Answer  0 :  {'text': 'أولئك الذين اشتروا الضلالة بالهدى', 'start_char': 504, 'end_char': 537}\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Let's compare the preprocessed and vanilla sets now set\n",
    "print(\"Before Preprocessing:\")\n",
    "print_set(train_passages, train_questions, train_answers)\n",
    "print(\"After Preprocessing:\")\n",
    "print_set(train_passages_prepro, train_questions_prepro, train_answers_prepro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38371d-a721-48cd-9caf-e2d1e32d169a",
   "metadata": {},
   "source": [
    "###  Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6809bfab-eca6-4ba4-b031-e90495589fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check If each list has any form of Empty Text or nulls, used for debugging errors with the dataset\n",
    "def check_if_nulls_exist(*lists):\n",
    "    #List to store tuples of (list_index, element_index, key) for dictionaries\n",
    "    none_indices = []  \n",
    "\n",
    "    #Iterate through each list\n",
    "    for list_index, current_list in enumerate(lists):\n",
    "        if not current_list:  #Handle empty lists\n",
    "            print(f\"List {list_index} is empty.\")\n",
    "            continue\n",
    "        \n",
    "        #Check if the current list is a list of strings\n",
    "        if isinstance(current_list[0], str):\n",
    "            for element_index, item in enumerate(current_list):\n",
    "                if item is None:\n",
    "                    none_indices.append((list_index, element_index, None))\n",
    "        \n",
    "        #Check if the current list is a list of dictionaries\n",
    "        elif isinstance(current_list[0], dict):\n",
    "            for dict_index, item in enumerate(current_list):\n",
    "                for key, value in item.items():\n",
    "                    if value is None:\n",
    "                        none_indices.append((list_index, dict_index, key))\n",
    "        \n",
    "        else:\n",
    "            print(f\"List {list_index} contains unsupported element types.\")\n",
    "\n",
    "    #Print results\n",
    "    if none_indices:\n",
    "        print(\"NoneType values found:\")\n",
    "        for list_index, element_index, key in none_indices:\n",
    "            if key is None:\n",
    "                print(f\"List {list_index}, Element {element_index}: NoneType value found.\")\n",
    "            else:\n",
    "                print(f\"List {list_index}, Dictionary {element_index}, Key '{key}':\", lists[list_index][element_index])\n",
    "    else:\n",
    "        print(\"No NoneType values found in the lists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9b39c9-61b2-4484-aa13-7668a9c49812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NoneType values found in the lists.\n",
      "No NoneType values found in the lists.\n"
     ]
    }
   ],
   "source": [
    "#check check NoneTypes for training set\n",
    "check_if_nulls_exist(train_passages_prepro, train_questions_prepro, train_answers_prepro)\n",
    "#check NoneTypes for validation set\n",
    "check_if_nulls_exist(val_passages_prepro, val_questions_prepro, val_answers_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f112b10-461f-4560-9fa6-0e2774e77282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function fixes the input list by converting a list of dictionaries to a list of strings if necessary.\n",
    "def dict_list_fixer(mylist):\n",
    "    #Check if the first element of the list is a dictionary\n",
    "    if type(mylist[0]) is dict:\n",
    "        #If it is, create a new list containing the values associated with the 'text' key in each dictionary\n",
    "        return [item.get('text') for item in mylist]\n",
    "    else:\n",
    "        #If it's not a dictionary (assumed to be a list of strings), return the list unchanged\n",
    "        return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945546df-f1f8-4c5a-8921-cc3247503910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Lengths of each Question, Passage, and Answer (Mean, Median, Mode, Maximum for both sets)\n",
    "def text_length_statistics(text_list):\n",
    "    #Fix the input list by converting it to a list of strings if it contains dictionaries\n",
    "    new_text_list = dict_list_fixer(text_list)\n",
    "    #Create a list of lengths of each text string\n",
    "    lengths = [len(text) for text in new_text_list]\n",
    "    #Calculate the minimum length\n",
    "    min_len = np.min(lengths)\n",
    "    #Calculate the maximum length\n",
    "    max_len = np.max(lengths)\n",
    "    #Calculate the mean (average) length\n",
    "    mean_len = np.mean(lengths)\n",
    "    #Calculate the median length\n",
    "    median_len = np.median(lengths)\n",
    "    \n",
    "    #Return the calculated statistics as a tuple\n",
    "    return min_len, max_len, mean_len, median_len\n",
    "\n",
    "#Prints out stats\n",
    "def print_text_length_statistics(text_list):\n",
    "    min_len, max_len, mean_len, median_len = text_length_statistics(text_list)\n",
    "    print(f\"Minimum Length: {min_len}\")\n",
    "    print(f\"Maximum Length: {max_len}\")\n",
    "    print(f\"Mean Length: {mean_len}\")\n",
    "    print(f\"Median Length: {median_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532dc324-554b-478c-a0c6-7f90d84dff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Train Passages Stats-----\n",
      "Minimum Length: 116\n",
      "Maximum Length: 1537\n",
      "Mean Length: 429.00929152148666\n",
      "Median Length: 387.0\n",
      "---- Train Questions Stats----\n",
      "Minimum Length: 11\n",
      "Maximum Length: 122\n",
      "Mean Length: 35.0\n",
      "Median Length: 35.0\n",
      "----- Train Answers Stats-----\n",
      "Minimum Length: 3\n",
      "Maximum Length: 1136\n",
      "Mean Length: 43.062717770034844\n",
      "Median Length: 27.0\n",
      "------------------------------\n",
      "\n",
      "----Validation Passages Stats----\n",
      "Minimum Length: 131\n",
      "Maximum Length: 863\n",
      "Mean Length: 441.3671875\n",
      "Median Length: 439.0\n",
      "----Validation Questions Stats----\n",
      "Minimum Length: 14\n",
      "Maximum Length: 53\n",
      "Mean Length: 27.890625\n",
      "Median Length: 25.0\n",
      "----Validation Answers Stats----\n",
      "Minimum Length: 5\n",
      "Maximum Length: 319\n",
      "Mean Length: 54.78125\n",
      "Median Length: 43.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Printing statistics for the datasets\n",
    "print(\"-----Train Passages Stats-----\")\n",
    "print_text_length_statistics(train_passages_prepro)\n",
    "print(\"---- Train Questions Stats----\")\n",
    "print_text_length_statistics(train_questions_prepro)\n",
    "print(\"----- Train Answers Stats-----\")\n",
    "print_text_length_statistics(train_answers_prepro)\n",
    "print(\"------------------------------\\n\")\n",
    "print(\"----Validation Passages Stats----\")\n",
    "print_text_length_statistics(val_passages_prepro)\n",
    "print(\"----Validation Questions Stats----\")\n",
    "print_text_length_statistics(val_questions_prepro)\n",
    "print(\"----Validation Answers Stats----\")\n",
    "print_text_length_statistics(val_answers_prepro)\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21f82b49-b469-4ba7-b5b9-1da552080741",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Plot histogram of text lengths\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_text_length_histogram\u001b[39m(text_list, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText Length Histogram\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\matplotlib\\__init__.py:172\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\matplotlib\\rcsetup.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\matplotlib\\colors.py:53\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPngImagePlugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PngInfo\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\PIL\\PngImagePlugin.py:44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IO, TYPE_CHECKING, Any, NoReturn\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageChops, ImageFile, ImagePalette, ImageSequence\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_binary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m i16be \u001b[38;5;28;01mas\u001b[39;00m i16\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_binary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m i32be \u001b[38;5;28;01mas\u001b[39;00m i32\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot histogram of text lengths\n",
    "def plot_text_length_histogram(text_list, title=\"Text Length Histogram\"):\n",
    "    new_text_list = dict_list_fixer(text_list)\n",
    "    lengths = [len(text) for text in new_text_list]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff720247-b2eb-43e2-b158-aa4bcde60839",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_histogram(train_passages_prepro, \"Train Passages Length Histogram\")\n",
    "plot_text_length_histogram(train_questions_prepro, \"Train Questions Length Histogram\")\n",
    "plot_text_length_histogram(train_answers_prepro, \"Train Answers Length Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80779710-a0b4-403d-96b2-b86dea6dd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_histogram(val_passages_prepro, \"Validation Passages Length Histogram\")\n",
    "plot_text_length_histogram(val_questions_prepro, \"Validation Questions Length Histogram\")\n",
    "plot_text_length_histogram(val_answers_prepro, \"Validation Answers Length Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae4db1-5d10-4dfc-9f89-cb887b92a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Possible Wordcloud Visualization Here (Not Required)------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gUsReZdrvQZ",
   "metadata": {
    "id": "_gUsReZdrvQZ"
   },
   "source": [
    "###  Text Tokenization/Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba2c50-55e7-40f6-8ea0-902861ff7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "#This function will help us determine the model type, which will come handy in selecting the appropriat tokenizer/model selection function\n",
    "def get_model_type(model_path):\n",
    "\n",
    "    #Load the model configuration\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    \n",
    "    #Return the model type\n",
    "    return config.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6e2d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "6581bb798dbf4d6d8486e6d8c5944f0b",
      "a9f6c51cbf5f403fa9107e0a0d63bf53",
      "a0bdb1425e3c44db8514a18d2eea4663",
      "c15c6717b560495fbf3b958a32fe3e87",
      "bdfdba7e7a174bf8b1b51b6041fcb7d5",
      "caf310f8e5dc436d9cd18ad9e6dda77a",
      "cfd58728c1c64242879e3cd0d424dcf7",
      "2630088666f34f5db5e02a0afd369b29",
      "53caf4b693084572ad9cb0cce751c1ca",
      "424c23a3930d482cab25093e195ab319",
      "a58d82a0ef144d0baea39f0fd3d22365",
      "b7fa0905c26343a6979a264fb53dacb2",
      "a7892941140f40cdb6a0d08d05e71dda",
      "5cde65fe85a448b0a86cbf8ea0c2a7ef",
      "2840ed9160c343dbb54d918c3be64624",
      "26db42ee704f4c5a87f23a638b6f991e",
      "c28c42f5e5b242aab645486d5db2623c",
      "8e95365fdbe044eda8aa4735cfefe007",
      "5def9acd41a341ebaa9baaca5bfe81a6",
      "c3547f0e6215473e99c9bffac63d57fb",
      "9910f922d9364c349b736726b17d64f4",
      "7bdab68e13034718b24f952d35e8790a",
      "abb21c1d50c44e49a03464a966922b2a",
      "5c452371f95f4adca3b64bd191aa1f02",
      "9cf9c13bc79e4fc382c354fe2a72d416",
      "f4f5040f35f74cf08b64ba5f4b2ed075",
      "7794613e7c2547059737fee6406e33f9",
      "cf5e0696b2bd4740a7e7c3e97f807a40",
      "99b1778434a34fbc981f6fb384012797",
      "2cea7303a0a3423d98308f7df3dbb2d9",
      "d30dc661d42a4794a5e4f06d84bd71a0",
      "0bf308066b194d43b6de1b53b284ab91",
      "074bab26bb3c43029575c4fc67f5137d",
      "d5359ee0b045420bb6202139f79c1445",
      "b7f1f951abe1454ea84d0c7703b68def",
      "952cde71642f4a39bc19090f340ecd56",
      "f2f9d499e4f54303bbb031705c181635",
      "cee92b770bfb4181ae84ef88a4621b60",
      "07e729be45284f28a3ee15be408e2d3e",
      "bc01d36956484be6abc2be865579de02",
      "ef27dcd62c2e4d7da22177a1f85eb992",
      "af0c86996560491f96e664a3ce0e2f48",
      "dff708c3c1db4a40abe1602778e4b839",
      "8d79d6bc8ae7456aab50ae7bc8b8f461",
      "0ef238c33f9842da9da7017d4571f114",
      "56beed392210489c801831cedec0bc6d",
      "7ec0fb5dffa94d4fa3f0342cbf91e7c9",
      "3cb8ccebe7254d0fba7c0d954b42192f",
      "3ac4592c4bc641a3aa2025e3c471adeb",
      "c6f27137b2264ecabc7ed6334fb9bce6",
      "10734bb2699045aca2be866650738c72",
      "e858ef86c45943e981f05b42622a24fa",
      "3337553386ce4938a7ac2e89edb1ae90",
      "573f6a9dbee241d59ce1b8561553d328",
      "037835305ba744119d025f5cf89e5b0d"
     ]
    },
    "id": "e7b6e2d1",
    "outputId": "dae1f437-11c4-4f7d-b14c-987d21195dd3"
   },
   "outputs": [],
   "source": [
    "#Importing our tokenizer methods\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, GPT2TokenizerFast, T5TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "#This function returns the appropriate tokenizer given the model's path\n",
    "def get_model_tokenizer(model_path):\n",
    "    model_type = get_model_type(model_path)\n",
    "    if model_type == 'bert':\n",
    "        return BertTokenizerFast.from_pretrained(model_path, model_max_length=512)\n",
    "    elif model_type == 't5':\n",
    "        return T5TokenizerFast.from_pretrained(model_path, model_max_length=512)\n",
    "    elif model_type == 'gpt2':\n",
    "        return GPT2TokenizerFast.from_pretrained(model_path, model_max_length=512)\n",
    "    else:\n",
    "        return AutoTokenizer.from_pretrained(model_path, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1faaeb-848f-44cc-ba58-5d641c9ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(tokenizer, passages, questions):\n",
    "    return tokenizer(passages, questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WkB5mZCYscAI",
   "metadata": {
    "id": "WkB5mZCYscAI"
   },
   "outputs": [],
   "source": [
    "#we will define and import AraBert V2 as our tokenizer for testing purposes\n",
    "model_path = 'aubmindlab/bert-base-arabertv2'\n",
    "tokenizer = get_model_tokenizer(model_path)\n",
    "\n",
    "#Now we need to create our encodings using the tokenizer we just initialized\n",
    "#What this will do is to actually merge those two strings together. So what we will have is our passage/context then a [SEP] token and then the question tokens\n",
    "#And this will be fed to AraBERT during training\n",
    "train_prepro_encodings = get_encodings(tokenizer,train_passages_prepro, train_questions_prepro)\n",
    "val_prepro_encodings = get_encodings(tokenizer,val_passages_prepro, val_questions_prepro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zp4EsPrntgem",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp4EsPrntgem",
    "outputId": "52f87edb-86da-49eb-e957-080d88fdf7d5"
   },
   "outputs": [],
   "source": [
    "#Now that our data has been converted to encoding objects, let's check them\n",
    "print(train_prepro_encodings.keys())\n",
    "print(\"input_ids: \" , train_prepro_encodings['input_ids'][0])\n",
    "print(\"input_tokens: \" , tokenizer.decode(train_prepro_encodings['input_ids'][0]))\n",
    "print(\"attention_mask: \", train_prepro_encodings['attention_mask'][0])\n",
    "len(train_prepro_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SYtyhQ_Fu7Ku",
   "metadata": {
    "id": "SYtyhQ_Fu7Ku"
   },
   "outputs": [],
   "source": [
    "#Next, we need to add start and end token positions to our encodings, because we don't have them in there yet\n",
    "def add_token_positions(encodings, answers):\n",
    "    #Initialize empty lists to store the start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    #Iterate over each answer\n",
    "    for i in range(len(answers)):\n",
    "        #Convert the character start position to a token start position and append to the list\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['start_char']))\n",
    "        #Convert the character end position to a token end position and append to the list\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['end_char']))\n",
    "\n",
    "        #If the start position is not found, then set it to the model's maximum length\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        #If the end position is not found, then set it to the previous character's token position\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['end_char'] - 1)\n",
    "\n",
    "    #Update the encodings with the calculated start and end positions\n",
    "    encodings.update({\n",
    "        'start_positions': start_positions,\n",
    "        'end_positions': end_positions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YnS4auml1zyl",
   "metadata": {
    "id": "YnS4auml1zyl"
   },
   "outputs": [],
   "source": [
    "#Add token positions to training, validation, and test encodings\n",
    "add_token_positions(train_prepro_encodings, train_answers_prepro)\n",
    "add_token_positions(val_prepro_encodings, val_answers_prepro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aF4pQsE01GNO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aF4pQsE01GNO",
    "outputId": "dff3110b-f456-4a19-dd99-10e3f65488b7"
   },
   "outputs": [],
   "source": [
    "print(train_prepro_encodings.keys())\n",
    "print(\"start_position: \" , train_prepro_encodings['start_positions'][0])\n",
    "print(\"end_position: \", train_prepro_encodings['end_positions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe01c9-81e7-487b-bcb9-42e589557604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function Check for NoneType values in a tokenizer encodings object\n",
    "def check_for_none_encodings(encodings):\n",
    "    none_indices = []\n",
    "    #Get the length of each list within the encodings dictionary\n",
    "    lengths = {key: len(value) for key, value in encodings.items()}\n",
    "\n",
    "    #Find the minimum length among all lists to prevent index out of range errors\n",
    "    min_length = min(lengths.values())\n",
    "\n",
    "    #Iterate over each key in the encodings dictionary\n",
    "    for key in encodings:\n",
    "        #Ensure we're not accessing indices beyond the length of the shortest list\n",
    "        for idx in range(min_length):\n",
    "            if encodings[key][idx] is None:\n",
    "                none_indices.append((idx, key))\n",
    "    \n",
    "    #Print out the indices where NoneType values are found, if any\n",
    "    if none_indices:\n",
    "        print(\"NoneType values found in encodings:\")\n",
    "        for idx, key in none_indices:\n",
    "            print(f\"Index {idx}, Key '{key}': {encodings[key][idx]}\")\n",
    "    else:\n",
    "        print(\"No NoneType values found in encodings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0b651-3825-4e59-abf5-84ed01037377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's check that our encodings are clean and have no NoneType values in order to make sure that they don't clash with the training & evaluation procedures\n",
    "check_for_none_encodings(train_prepro_encodings)\n",
    "check_for_none_encodings(val_prepro_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qEAbWQt42jPd",
   "metadata": {
    "id": "qEAbWQt42jPd"
   },
   "outputs": [],
   "source": [
    "#Okay, so our data is in the right format at the moment. Now let's create a PyTorch dataset object using it!\n",
    "import torch\n",
    "\n",
    "#We then define the dataset using a class\n",
    "class QuranDataset(torch.utils.data.Dataset):\n",
    "    #This is an initializing function, similar to constructors in Java and C++\n",
    "    def __init__(self, encodings):\n",
    "        #Store the encodings as an instance variable\n",
    "        self.encodings = encodings\n",
    "\n",
    "    #This function allows the dataset to be indexed\n",
    "    def __getitem__(self, idx):\n",
    "        #Return a dictionary where each key-value pair is converted to a tensor\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    #This function returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        #Return the number of input_ids in the encodings\n",
    "        return len(self.encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ZCxjrJY374e",
   "metadata": {
    "id": "5ZCxjrJY374e"
   },
   "outputs": [],
   "source": [
    "#So we apply this to our encodings to create datasets objects\n",
    "train_prepro_dataset = QuranDataset(train_prepro_encodings)\n",
    "val_prepro_dataset = QuranDataset(val_prepro_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633574e-732e-40a4-aa55-9c5d2cf64c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function checks for NoneType values in a PyTorch dataset object, We need to do that in order to make sure that our dataset implementation is valid\n",
    "#This will save us alot of headache during model training errors debugging\n",
    "def check_for_none_pytorch_dataset(dataset):\n",
    "    #Initialize an empty list to store indices and keys where NoneType values are found\n",
    "    none_indices = []  \n",
    "    \n",
    "    #Loop through each index in the dataset\n",
    "    for idx in range(len(dataset)):\n",
    "        item = dataset[idx]  #Retrieve the item (a dictionary) at the current index\n",
    "        #Loop through each key-value pair in the dictionary\n",
    "        for key, value in item.items():\n",
    "            #Check if the value is None\n",
    "            if value is None:\n",
    "                #If the value is None, append a tuple of the index and the key to the none_indices list\n",
    "                none_indices.append((idx, key))\n",
    "    \n",
    "    #Check if there are any NoneType values recorded in none_indices\n",
    "    if none_indices:\n",
    "        #If NoneType values are found, print a message indicating this\n",
    "        print(\"NoneType values found in PyTorch dataset:\")\n",
    "        #Loop through the list of indices and keys where None values were found\n",
    "        for idx, key in none_indices:\n",
    "            #Print the index, key, and the item at the index to show the context\n",
    "            print(f\"Index {idx}, Key '{key}':\", dataset[idx])\n",
    "    else:\n",
    "        #If no NoneType values are found, print a message indicating this\n",
    "        print(\"No NoneType values found in PyTorch dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e7c0b-5f46-4c8e-83d9-b2ca28678704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the datasets objects  has any form of NoneTypes\n",
    "check_for_none_pytorch_dataset(train_prepro_dataset)\n",
    "check_for_none_pytorch_dataset(val_prepro_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc265a-4112-4947-a3d6-a6fa47290d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, We integrate the entire process into a single handy & easy to use function\n",
    "#This function converts input data into a PyTorch dataset format suitable for model training or evaluation\n",
    "def dataset_to_pytorch_format(passages_list, questions_list, answers_list, model_path='aubmindlab/bert-base-arabertv2'):\n",
    "    \n",
    "    #Get the tokenizer for the specified model\n",
    "    tokenizer = get_model_tokenizer(model_path)\n",
    "    if get_model_type(model_path) == 'gpt2':\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    #Tokenize passages and questions to get encodings\n",
    "    encodings = get_encodings(tokenizer, passages_list, questions_list)\n",
    "    #Add token positions to the encodings based on the answers\n",
    "    add_token_positions(encodings, answers_list)\n",
    "    #Create a PyTorch dataset object from the encodings\n",
    "    my_pytorch_dataset_object = QuranDataset(encodings)\n",
    "    #Return the PyTorch dataset object\n",
    "    return my_pytorch_dataset_object, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gxCxuV-I47Ay",
   "metadata": {
    "id": "gxCxuV-I47Ay"
   },
   "source": [
    "###  Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGSGkyR5m_m5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "c3d8ef1aae4b447899332e3f5eaeb09d",
      "f85778a72149439a899ad89d5f0b3163",
      "2b7055cd8c814162b1b39c4e885c6bb0",
      "33e4c43a798b4af09d7931b180fac455",
      "04dfd6992af846d5abb51cd1266db7cc",
      "05af80238f6f41249b7842dccc0b82cc",
      "7a8aab4bbe7c4c55b24f53b375c1576b",
      "feaae86c965a4948b853bd7e3ddf7f40",
      "6f6449ca3b95470b85b07d56acecda34",
      "e7889cc182874533a9e574cc701ba331",
      "4215fa3b2ed94bdc83fc93803326ede3"
     ]
    },
    "id": "lGSGkyR5m_m5",
    "outputId": "c8a9b990-1a61-41f2-e833-20e0d510de3c"
   },
   "outputs": [],
   "source": [
    "#Initializing the model to be used. We will use a pretrained model known as AraBERT\n",
    "from transformers import BertForQuestionAnswering, T5ForQuestionAnswering, GPT2ForQuestionAnswering, AutoModelForQuestionAnswering\n",
    "\n",
    "def get_model(model_path):\n",
    "    model_type = get_model_type(model_path)\n",
    "    if model_type == 'bert':\n",
    "        return BertForQuestionAnswering.from_pretrained(model_path)\n",
    "    elif model_type == 't5':\n",
    "        return T5ForQuestionAnswering.from_pretrained(model_path)\n",
    "    elif model_type == 'gpt2':\n",
    "        return GPT2ForQuestionAnswering.from_pretrained(model_path)\n",
    "    else:\n",
    "        return AutoModelForQuestionAnswering.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V_l-d3wl4tBr",
   "metadata": {
    "id": "V_l-d3wl4tBr"
   },
   "outputs": [],
   "source": [
    "#Now we have to load all of that into a dataloader object\n",
    "from torch.utils.data import DataLoader\n",
    "#We use Adam optimizer with weight decay to reduce the chances of over-fitting.\n",
    "from transformers import AdamW\n",
    "#This is the progress bar library\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795a2ee-0cbc-43a7-8050-f2579807c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop( model, optimizer, device, train_loader, epochs):\n",
    "    #Loop for training the model for a total of four epochs\n",
    "    for epoch in range(epochs):\n",
    "        #Initialize the progress bar with the training data loader\n",
    "        progress_bar = tqdm(train_loader)\n",
    "        \n",
    "        #Iterate over each batch of data from the training data loader\n",
    "        for batch in progress_bar:\n",
    "            #Reset the gradients from the previous iteration to zero\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Move input data and labels to the specified device (CPU/GPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "    \n",
    "            #Forward pass: compute the model's output for the given inputs\n",
    "            outputs = model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "            \n",
    "            #Extract the loss from the model's output (assuming the model returns the loss as the first element)\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            #Backward pass: compute the gradients of the loss with respect to the model parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            #Update the model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "    \n",
    "            #Update the progress bar with the current epoch number and the current loss\n",
    "            progress_bar.set_description(f'Epoch {epoch}')\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbf4c5-3862-4e28-8014-a3e02bd345bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_path, train_dataset, batch_size=2, learning_rate=2e-5, epochs=3):\n",
    "    model = get_model(model_path)\n",
    "    #Setting our target device for training and other stuff. GPU is the default.\n",
    "    torch.cuda.empty_cache()\n",
    "    #Transferring model to our target hardware device\n",
    "    model.to(device)\n",
    "    #Selecting train mode for the model\n",
    "    model.train()\n",
    "    #Initialize the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    #Now, let's initialize our dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    training_loop(model, optimizer, device, train_loader, epochs=epochs)\n",
    "    #returns the model after being trained\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f0833-2d8f-4735-9be0-147ce5d03f6f",
   "metadata": {},
   "source": [
    "#### Grid Search Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c46a0-5e25-4f66-989b-e1cedef85c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Custom class inheriting from BaseEstimator and ClassifierMixin for integration with scikit-learn\n",
    "class CustomTransformer(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_path, batch_size=2, learning_rate=2e-5, epochs=3):\n",
    "        self.model_path = model_path  #Path to the pre-trained model\n",
    "        self.batch_size = batch_size  #Batch size for training\n",
    "        self.learning_rate = learning_rate  #Learning rate for the optimizer\n",
    "        self.epochs = epochs  #Number of training epochs\n",
    "        self.model = None  #Placeholder for the model\n",
    "\n",
    "    #Method to train the model\n",
    "    def fit(self, X, y=None):\n",
    "        #Prepare the dataset and dataloader\n",
    "        train_dataset = self.prepare_dataset(X, y)\n",
    "        self.model = get_model(self.model_path)  #Load the pre-trained model\n",
    "        \n",
    "        #Setting up the device (GPU if available, else CPU)\n",
    "        torch.cuda.empty_cache()  #Clear the cache\n",
    "        self.model.to(device)  #Move the model to the device\n",
    "        self.model.train()  #Set the model to training mode\n",
    "        \n",
    "        #Initialize the optimizer\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        #Initialize the dataloader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        #Training loop\n",
    "        self.training_loop(self.model, optimizer, device, train_loader, self.epochs)\n",
    "        return self\n",
    "    \n",
    "    #Method to score the model (dummy implementation)\n",
    "    def score(self, X, y=None):\n",
    "        #Implement your evaluation logic here\n",
    "        #For demonstration, we'll return a dummy score\n",
    "        return 1.0\n",
    "\n",
    "    #Method to prepare the dataset (placeholder implementation)\n",
    "    def prepare_dataset(self, X, y):\n",
    "        #Prepare and return your dataset\n",
    "        return X\n",
    "        \n",
    "    #Training loop implementation\n",
    "    def training_loop(self, model, optimizer, device, train_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            progress_bar = tqdm(train_loader)  #Initialize the progress bar\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()  #Reset gradients from the previous iteration\n",
    "                #Move batch data to the device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "                #Forward pass: compute model output\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "                loss = outputs[0]  #Extract the loss\n",
    "                loss.backward()  #Backward pass: compute gradients\n",
    "                optimizer.step()  #Update model parameters\n",
    "                #Update progress bar description and loss\n",
    "                progress_bar.set_description(f'Epoch {epoch}')\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa00e96-4a88-42a8-ac76-bad6e948f744",
   "metadata": {},
   "source": [
    "#### Model Evaluation & Accuracy Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd902c8c-df25-49a9-9ff7-610f5d260333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Function to calculate various metrics\n",
    "def calculate_metrics(start_preds, end_preds, start_positions, end_positions):\n",
    "    #Convert tensors to numpy arrays and move them to CPU\n",
    "    start_preds = start_preds.cpu().numpy()\n",
    "    end_preds = end_preds.cpu().numpy()\n",
    "    start_positions = start_positions.cpu().numpy()\n",
    "    end_positions = end_positions.cpu().numpy()\n",
    "\n",
    "    #Ensure arrays are at least 1D and flatten them\n",
    "    start_preds = np.atleast_1d(start_preds.flatten())\n",
    "    end_preds = np.atleast_1d(end_preds.flatten())\n",
    "    start_positions = np.atleast_1d(start_positions.flatten())\n",
    "    end_positions = np.atleast_1d(end_positions.flatten())\n",
    "\n",
    "    #Calculate start position metrics\n",
    "    accuracy_start = accuracy_score(start_positions, start_preds)\n",
    "    precision_start = precision_score(start_positions, start_preds, average='macro')\n",
    "    recall_start = recall_score(start_positions, start_preds, average='macro')\n",
    "    f1_start = f1_score(start_positions, start_preds, average='macro')\n",
    "\n",
    "    #Calculate end position metrics\n",
    "    accuracy_end = accuracy_score(end_positions, end_preds)\n",
    "    precision_end = precision_score(end_positions, end_preds, average='macro')\n",
    "    recall_end = recall_score(end_positions, end_preds, average='macro')\n",
    "    f1_end = f1_score(end_positions, end_preds, average='macro')\n",
    "\n",
    "    #Calculate the average of start and end position metrics\n",
    "    accuracy = (accuracy_start + accuracy_end) / 2\n",
    "    precision = (precision_start + precision_end) / 2\n",
    "    recall = (recall_start + recall_end) / 2\n",
    "    f1 = (f1_start + f1_end) / 2\n",
    "    \n",
    "    #Function to calculate Mean Reciprocal Rank (MRR)\n",
    "    def reciprocal_rank(start_pred, start_pos):\n",
    "        ranks = np.where(start_pos == start_pred)[0]\n",
    "        return 1 / (ranks[0] + 1) if ranks.size > 0 else 0\n",
    "\n",
    "    #Calculate MRR\n",
    "    mrr = np.mean([reciprocal_rank(sp, spos) for sp, spos in zip(start_preds, start_positions)])\n",
    "\n",
    "    #Function to calculate Precision at Rank (pRR)\n",
    "    def precision_at_rank(start_pred, start_pos):\n",
    "        return 1 if start_pred == start_pos else 0\n",
    "\n",
    "    #Calculate pRR\n",
    "    prr = np.mean([precision_at_rank(sp, spos) for sp, spos in zip(start_preds, start_positions)])\n",
    "\n",
    "    #Return all calculated metrics\n",
    "    return accuracy, precision, recall, f1, mrr, prr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa464c-bad2-4e2e-9025-0fca4246c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, val_dataset, batch_size, device):\n",
    "    #Create a DataLoader for the validation dataset\n",
    "    validation_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    #Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    #Move the model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    #Initialize variables to store total loss and predictions\n",
    "    total_loss = 0\n",
    "    all_start_preds = []\n",
    "    all_end_preds = []\n",
    "    all_start_positions = []\n",
    "    all_end_positions = []\n",
    "\n",
    "    #Disable gradient calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        #Iterate over the validation data in batches\n",
    "        for batch in validation_loader:\n",
    "            #Move input data and labels to the specified device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            #Forward pass through the model to get predictions and loss\n",
    "            outputs = model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "\n",
    "            #Get the loss from the model's output and accumulate it\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #Get the start and end logits from the model's output\n",
    "            start_logits, end_logits = outputs[1], outputs[2]\n",
    "            #Convert logits to predicted positions by taking the argmax\n",
    "            start_preds = torch.argmax(start_logits, dim=1)\n",
    "            end_preds = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            #Store the predictions and actual positions\n",
    "            all_start_preds.append(start_preds)\n",
    "            all_end_preds.append(end_preds)\n",
    "            all_start_positions.append(start_positions)\n",
    "            all_end_positions.append(end_positions)\n",
    "\n",
    "    #Calculate the average loss over all validation batches\n",
    "    avg_loss = total_loss / len(validation_loader)\n",
    "    \n",
    "    #Calculate evaluation metrics using the predictions and actual positions\n",
    "    accuracy, precision, recall, f1, mrr, prr = calculate_metrics(\n",
    "        torch.cat(all_start_preds),  #Concatenate all start predictions\n",
    "        torch.cat(all_end_preds),    #Concatenate all end predictions\n",
    "        torch.cat(all_start_positions),  #Concatenate all actual start positions\n",
    "        torch.cat(all_end_positions)     #Concatenate all actual end positions\n",
    "    )\n",
    "\n",
    "    #Return the average loss and evaluation metrics\n",
    "    return avg_loss, accuracy, precision, recall, f1, mrr, prr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9ff8f-3b5a-499f-ad65-e74298482831",
   "metadata": {},
   "source": [
    "### AraBERT V2 Training, Evaluation, & Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984d4f6-a749-4406-bdde-965ad3dbbeaf",
   "metadata": {},
   "source": [
    "#### Model Training Without Preprocessing Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbff64-5d10-4fcb-9251-4c7a6c94606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "model_path = 'aubmindlab/bert-base-arabertv2'\n",
    "#first let's compare the non-preprocessed(vanilla) dataset vs the preprocessed datasets on AraBert V2\n",
    "train_dataset_vanilla, tokenizer = dataset_to_pytorch_format(train_passages,train_questions, train_answers, model_path)\n",
    "#Training using those default parameters\n",
    "trained_model_vanilla = train_model(model_path, train_dataset_vanilla, 2, 2e-5, 5)\n",
    "model_path = 'model/araBert-quranQA-v2-vanilla'\n",
    "trained_model_vanilla.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e7545-cc06-4378-bec5-0a2f8df3654c",
   "metadata": {},
   "source": [
    "#### Model Training With Preprocessing Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1685ee1-7272-4311-8a30-b0862e42b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'aubmindlab/bert-base-arabertv2'\n",
    "\n",
    "train_dataset_prepro, tokenizer = dataset_to_pytorch_format(train_passages_prepro,train_questions_prepro, train_answers_prepro, model_path)\n",
    "#Training using those default parameters\n",
    "trained_model_prepro = train_model(model_path, train_dataset_prepro, 2, 2e-5, 5)\n",
    "model_path = 'model/araBert-quranQA-v2-preprocessed'\n",
    "trained_model_prepro.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118b237-b97b-4822-b334-6780baeeb3fa",
   "metadata": {},
   "source": [
    "#### Evaluating both models on their respective evaluation sets to see their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d11d2-3668-4d68-9f77-cf8bf75ff220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/araBert-quranQA-v2-vanilla'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "val_dataset_vanilla, temp = dataset_to_pytorch_format(val_passages, val_questions, val_answers, model_path)\n",
    "\n",
    "#Evaluate the model on the validation set\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_f1, val_mrr, val_prr = evaluate(model, val_dataset_vanilla, batch_size=2, device=device)\n",
    "print(\"--------------Arabert V2 Trained on Vanilla QRCD Dataset --------------\")\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(f'Validation Precision: {val_precision}')\n",
    "print(f'Validation Recall: {val_recall}')\n",
    "print(f'Validation F1 Score: {val_f1}')\n",
    "print(f'Validation MRR: {val_mrr}')\n",
    "print(f'Validation pRR: {val_prr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734f813-b29c-4ee7-ba1c-267311ca415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/araBert-quranQA-v2-preprocessed/'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "val_dataset_prepro, temp = dataset_to_pytorch_format(val_passages_prepro, val_questions_prepro, val_answers_prepro, model_path)\n",
    "\n",
    "#Evaluate the model on the validation set\n",
    "val_prepro_loss, val_prepro_accuracy, val_prepro_precision, val_prepro_recall, val_prepro_f1, val_prepro_mrr, val_prepro_prr = evaluate(model, val_dataset_prepro, batch_size=2, device=device)\n",
    "print(\"--------------Arabert V2 Trained on Preprocessed QRCD Dataset --------------\")\n",
    "print(f'Validation Loss: {val_prepro_loss}')\n",
    "print(f'Validation Accuracy: {val_prepro_accuracy}')\n",
    "print(f'Validation Precision: {val_prepro_precision}')\n",
    "print(f'Validation Recall: {val_prepro_recall}')\n",
    "print(f'Validation F1 Score: {val_prepro_f1}')\n",
    "print(f'Validation MRR: {val_prepro_mrr}')\n",
    "print(f'Validation pRR: {val_prepro_prr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24187b2-8460-432f-8475-1b331132dd67",
   "metadata": {},
   "source": [
    "### MarBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0956d9-34b3-467c-9124-345d4e1c7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'UBC-NLP/MARBERT'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_dataset, tokenizer = dataset_to_pytorch_format(train_passages,train_questions, train_answers, model_path)\n",
    "#Training using those default parameters\n",
    "model = train_model(model_path, train_dataset, 2, 2e-5, 5)\n",
    "model_path = 'model/MARBERT-quranQA'\n",
    "\n",
    "\n",
    "\n",
    "def make_contiguous(state_dict):\n",
    "    for key in state_dict:\n",
    "        if not state_dict[key].is_contiguous():\n",
    "            state_dict[key] = state_dict[key].contiguous()\n",
    "    return state_dict\n",
    "\n",
    "# Assume `model` is your trained BERT model\n",
    "model_path = 'model/MARBERT-quranQA'\n",
    "\n",
    "# Make all tensors in the state_dict contiguous\n",
    "state_dict = model.state_dict()\n",
    "state_dict = make_contiguous(state_dict)\n",
    "\n",
    "# Save the modified state_dict\n",
    "model.save_pretrained(model_path, state_dict=state_dict)\n",
    "\n",
    "# Save the tokenizer if needed\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddf5f7-6511-4d02-bb81-de0be5fe6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/MARBERT-quranQA'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "val_dataset, temp = dataset_to_pytorch_format(val_passages, val_questions, val_answers, model_path)\n",
    "\n",
    "#Evaluate the model on the validation set\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_f1, val_mrr, val_prr = evaluate(model, val_dataset, batch_size=2, device=device)\n",
    "print(\"--------------MARBERT Trained on Vanilla QRCD Dataset --------------\")\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(f'Validation Precision: {val_precision}')\n",
    "print(f'Validation Recall: {val_recall}')\n",
    "print(f'Validation F1 Score: {val_f1}')\n",
    "print(f'Validation MRR: {val_mrr}')\n",
    "print(f'Validation pRR: {val_prr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca600b-5056-4a84-b81e-c70f4eab4086",
   "metadata": {},
   "source": [
    "#### Testing Grid Search on AraBert V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e0334-4bb5-4141-93f6-5f2bd63f85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameter grid for Grid Search\n",
    "#This dictionary specifies the hyperparameters to be tested during the grid search\n",
    "#'batch_size' will be tested with values 2, 4, and 6\n",
    "#'learning_rate' will be tested with values 2e-5, 1e-4, and 2e-4\n",
    "#'epochs' will be tested with values 2, 5, and 10\n",
    "torch.cuda.empty_cache()\n",
    "param_grid = {\n",
    "    'batch_size': [2, 4, 6],\n",
    "    'learning_rate': [2e-5, 1e-4, 2e-4],\n",
    "    'epochs': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Define the path to the pre-trained model\n",
    "model_path = 'aubmindlab/bert-base-arabertv2'\n",
    "\n",
    "#Convert the training dataset (passages, questions, answers) into PyTorch format\n",
    "#This function also returns the tokenizer (unused here)\n",
    "train_dataset, tokenizer = dataset_to_pytorch_format(train_passages, train_questions, train_answers, model_path)\n",
    "\n",
    "#Initialize GridSearchCV with the custom transformer model and the parameter grid\n",
    "#GridSearchCV will perform an exhaustive search over the specified parameter grid\n",
    "#It evaluates the model using cross-validation to find the best combination of hyperparameters\n",
    "grid_search = GridSearchCV(CustomTransformer(model_path), param_grid)\n",
    "\n",
    "#Fit GridSearchCV to the training dataset\n",
    "#This process will train the model with all combinations of hyperparameters from the parameter grid\n",
    "grid_search.fit(train_dataset)\n",
    "\n",
    "#Print the best hyperparameters found during the grid search\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "#Print the best score achieved with the best hyperparameters\n",
    "print(\"Best score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a64227-262b-42d9-b024-bd40da6ae26c",
   "metadata": {},
   "source": [
    "#### Testing Our Pipeline on another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0081a50-dcdb-410e-a96a-c257f9bd8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#Since I don't have enough Vram, then let's use the validation set as training samples number by in order to see if the pipeline can work with other models\n",
    "torch.cuda.empty_cache()\n",
    "model_path = 'UBC-NLP/AraT5-base'\n",
    "val_dataset_vanilla, tokenizer = dataset_to_pytorch_format(val_passages, val_questions, val_answers, model_path)\n",
    "trained_model_vanilla = train_model(model_path, val_dataset_vanilla, 2, 2e-5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc084ab5-1252-4b68-9d71-6d0c88b9ea6a",
   "metadata": {},
   "source": [
    "### Predicting An Answer Using a QRCD Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0909737-cf41-4c80-9464-3782fb0849c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's predict an answer using a context, question pair of our own\n",
    "#Load the fine-tuned model and tokenizer\n",
    "model_path = 'model/araBert-quranQA-v0.5/'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "#Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#Define the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "#Function to predict the answer from context and question\n",
    "def predict_answer(context, question):\n",
    "    #Tokenize the input context and question\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "    \n",
    "    #Move inputs to the specified device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    #Perform a forward pass to get the model's output for the given inputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    #Get the predicted start and end positions\n",
    "    start_logits = outputs['start_logits']\n",
    "    end_logits = outputs['end_logits']\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "    \n",
    "    #Decode the predicted answer from the input ids\n",
    "    answer_ids = input_ids[0][start_idx:end_idx+1]\n",
    "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "#Example usage\n",
    "context = input(\"ضع النص القرأني هنا:\")\n",
    "question = input(\"ضع سؤالك هنا:\")\n",
    "predicted_answer = predict_answer(context, question)\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82188e5-720b-49e7-8406-f1e52582d860",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28fb49-e68a-4552-b81a-76887dea4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing the result\n",
    "#Extract results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "#Create a plot of the grid search results\n",
    "learning_rates = param_grid['learning_rate']\n",
    "mean_scores = results['mean_test_score']\n",
    "std_scores = results['std_test_score']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(learning_rates, mean_scores, yerr=std_scores, fmt='o', capsize=5)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.title('Grid Search Results')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b24ed-a59b-47dc-bd24-1fa708f0abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings Extraction\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "#Load pre-trained model and tokenizer\n",
    "model_name = 'model/araBert-quranQA-v2-vanilla'  #You can change this to 'aubmindlab/bert-base-arabertv2' or any other model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "#Ensure the model is in evaluation mode and moved to the appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_embeddings(texts, tokenizer, model, device):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        #Tokenize input text and convert to PyTorch tensors\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        #Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            #Get the embeddings for the [CLS] token (assuming BERT or similar model)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "#Generate embeddings for each list\n",
    "passage_embeddings = generate_embeddings(train_passages, tokenizer, model, device)\n",
    "question_embeddings = generate_embeddings(train_questions, tokenizer, model, device)\n",
    "answer_embeddings = generate_embeddings([answer['text'] for answer in train_answers], tokenizer, model, device)\n",
    "\n",
    "#Print the shape of the embeddings\n",
    "print(f\"Passage Embeddings Shape: {len(passage_embeddings)}, {passage_embeddings[0].shape}\")\n",
    "print(f\"Question Embeddings Shape: {len(question_embeddings)}, {question_embeddings[0].shape}\")\n",
    "print(f\"Answer Embeddings Shape: {len(answer_embeddings)}, {answer_embeddings[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59096a-390f-4dc4-9178-7ed8e0bd2499",
   "metadata": {},
   "source": [
    "### Graphical User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6115d9-ecef-4b56-85e3-be917565bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "def get_answer_from_model(model_path, passage, question):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "    #Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #Define the device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    #Tokenize the input context and question\n",
    "    inputs = tokenizer.encode_plus(question, passage, return_tensors='pt')\n",
    "    \n",
    "    #Move inputs to the specified device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    #Perform a forward pass to get the model's output for the given inputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    #Get the predicted start and end positions\n",
    "    start_logits = outputs['start_logits']\n",
    "    end_logits = outputs['end_logits']\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "    \n",
    "    #Decode the predicted answer from the input ids\n",
    "    answer_ids = input_ids[0][start_idx:end_idx+1]\n",
    "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aafa970-b74b-4d26-b73b-b068c2fb807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, Canvas, Entry, Button, PhotoImage, StringVar, OptionMenu, Text, Scrollbar, Frame\n",
    "\n",
    "model_name = \"\"\n",
    "\n",
    "def open_popup(passage, question):\n",
    "    global popup, text_widget\n",
    "    \n",
    "    window.destroy()\n",
    "    \n",
    "    # Create a new window\n",
    "    popup = Tk()\n",
    "    popup.title(\"Popup Window\")\n",
    "    popup.geometry(\"640x480\")\n",
    "    \n",
    "    # Create a StringVar for the dropdown menu\n",
    "    selected_option = StringVar(popup)\n",
    "    selected_option.set(\"AraBERT V0.2\")  # Set default option\n",
    "    \n",
    "    # Create a dropdown menu\n",
    "    options = [\"AraBERT V0.2\", \"AraBERT V2 (Buggy)\", \"MARBERT\"]\n",
    "    dropdown_menu = OptionMenu(popup, selected_option, *options, command=option_selected)\n",
    "    dropdown_menu.pack(pady=10)\n",
    "    \n",
    "    global model_name  \n",
    "    model_name = 'model/araBert-quranQA-v0.2'\n",
    "    \n",
    "    # Create a frame to hold the Text widget and scrollbar\n",
    "    text_frame = Frame(popup)\n",
    "    text_frame.pack(pady=10, padx=10, fill='both', expand=True)\n",
    "    \n",
    "    # Create a Text widget for multi-line text display\n",
    "    text_widget = Text(text_frame, wrap='word', bg=\"#D9D9D9\", fg=\"#000716\", bd=0, height=15)\n",
    "    text_widget.config(state='disabled')  # Make the Text widget read-only\n",
    "    \n",
    "    # Create a vertical scrollbar\n",
    "    scrollbar = Scrollbar(text_frame, command=text_widget.yview)\n",
    "    text_widget.config(yscrollcommand=scrollbar.set)\n",
    "    \n",
    "    # Pack the Text widget and scrollbar\n",
    "    text_widget.pack(side='left', fill='both', expand=True)\n",
    "    scrollbar.pack(side='right', fill='y')\n",
    "    \n",
    "    # Create a button that displays text\n",
    "    display_button = Button(popup, text=\"Display Text\", command=lambda: display_text(popup, get_answer_from_model(model_name, passage, question)))\n",
    "    display_button.pack(pady=10)\n",
    "\n",
    "    # Define on_closing function\n",
    "    def on_closing():\n",
    "        popup.destroy()\n",
    "        view_main_window()\n",
    "    \n",
    "    popup.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "def option_selected(option):\n",
    "    global model_name  \n",
    "    if option == \"AraBERT V0.2\":\n",
    "        model_name = 'model/araBert-quranQA-v0.2'\n",
    "    elif option == \"AraBERT V2 (Buggy)\":\n",
    "        model_name = 'model/araBert-quranQA-v2-vanilla'\n",
    "    elif option == \"MARBERT\":\n",
    "        model_name = 'model/MARBERT-quranQA'\n",
    "\n",
    "def display_text(popup, text):\n",
    "    global text_widget\n",
    "    \n",
    "    # Clear the existing text in the Text widget\n",
    "    text_widget.config(state='normal')  # Allow editing\n",
    "    text_widget.delete('1.0', 'end')  # Clear all text\n",
    "    \n",
    "    # Insert the new text\n",
    "    text_widget.insert('1.0', text)\n",
    "    text_widget.config(state='disabled')  # Make the Text widget read-only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127e61ff-a1ff-420e-8506-7b51ea6a8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# from tkinter import *\n",
    "# Explicit imports to satisfy Flake8\n",
    "\n",
    "\n",
    "# Determine the root directory\n",
    "if '__file__' in globals():\n",
    "    ROOT_DIR = Path(__file__).parent\n",
    "else:\n",
    "    # Fallback to current working directory\n",
    "    ROOT_DIR = Path(os.getcwd())\n",
    "\n",
    "# Define the assets directory relative to the root directory\n",
    "ASSETS_PATH = ROOT_DIR / Path(\"assets/frame0\")\n",
    "\n",
    "def relative_to_assets(path: str) -> Path:\n",
    "    return ASSETS_PATH / Path(path)\n",
    "\n",
    "def view_main_window():\n",
    "    global window\n",
    "    window = Tk()\n",
    "\n",
    "    window.geometry(\"632x455\")\n",
    "    window.configure(bg=\"#FFFFFF\")\n",
    "    \n",
    "    canvas = Canvas(\n",
    "        window,\n",
    "        bg=\"#FFFFFF\",\n",
    "        height=455,\n",
    "        width=632,\n",
    "        bd=0,\n",
    "        highlightthickness=0,\n",
    "        relief=\"ridge\"\n",
    "    )\n",
    "    canvas.place(x=0, y=0)\n",
    "    \n",
    "    canvas.create_rectangle(\n",
    "        0.0,\n",
    "        0.0,\n",
    "        632.0,\n",
    "        455.0,\n",
    "        fill=\"#FFFFFF\",\n",
    "        outline=\"\"\n",
    "    )\n",
    "    \n",
    "    image_image_1 = PhotoImage(file=relative_to_assets(\"image_1.png\"))\n",
    "    image_1 = canvas.create_image(\n",
    "        316.0,\n",
    "        36.0,\n",
    "        image=image_image_1\n",
    "    )\n",
    "    \n",
    "    canvas.create_text(\n",
    "        0.0,\n",
    "        20.0,\n",
    "        anchor=\"nw\",\n",
    "        text=\"نظام سؤال وجواب في كتاب اللَّه الكريم\",\n",
    "        fill=\"#000000\",\n",
    "        font=(\"AnonymousPro Regular\", 20 * -1)\n",
    "    )\n",
    "    \n",
    "    entry_image_1 = PhotoImage(file=relative_to_assets(\"entry_1.png\"))\n",
    "    entry_bg_1 = canvas.create_image(\n",
    "        316.0,\n",
    "        223.5,\n",
    "        image=entry_image_1\n",
    "    )\n",
    "    entry_1 = Text(\n",
    "        window,\n",
    "        bd=0,\n",
    "        bg=\"#D9D9D9\",\n",
    "        fg=\"#000716\",\n",
    "        highlightthickness=0\n",
    "    )\n",
    "    entry_1.place(\n",
    "        x=43.0,\n",
    "        y=127.0,\n",
    "        width=546.0,\n",
    "        height=191.0\n",
    "    )\n",
    "    \n",
    "    canvas.create_text(\n",
    "        33.0,\n",
    "        93.0,\n",
    "        anchor=\"nw\",\n",
    "        text=\"قم بادخال النص القرآني هنا\",\n",
    "        fill=\"#000000\",\n",
    "        font=(\"AnonymousPro Regular\", 20 * -1)\n",
    "    )\n",
    "    \n",
    "    canvas.create_text(\n",
    "        33.0,\n",
    "        333.0,\n",
    "        anchor=\"nw\",\n",
    "        text=\"قم بادخال السؤال هنا\",\n",
    "        fill=\"#000000\",\n",
    "        font=(\"AnonymousPro Regular\", 20 * -1)\n",
    "    )\n",
    "    \n",
    "    entry_image_2 = PhotoImage(file=relative_to_assets(\"entry_2.png\"))\n",
    "    entry_bg_2 = canvas.create_image(\n",
    "        316.0,\n",
    "        379.5,\n",
    "        image=entry_image_2\n",
    "    )\n",
    "    entry_2 = Entry(\n",
    "        window,\n",
    "        bd=0,\n",
    "        bg=\"#D9D9D9\",\n",
    "        fg=\"#000716\",\n",
    "        highlightthickness=0\n",
    "    )\n",
    "    entry_2.place(\n",
    "        x=43.0,\n",
    "        y=367.0,\n",
    "        width=546.0,\n",
    "        height=23.0\n",
    "    )\n",
    "    \n",
    "    button_image_1 = PhotoImage(file=relative_to_assets(\"button_1.png\"))\n",
    "    button_1 = Button(\n",
    "        window,\n",
    "        image=button_image_1,\n",
    "        borderwidth=0,\n",
    "        highlightthickness=0,\n",
    "        command=lambda: open_popup(entry_1.get(\"1.0\", \"end-1c\"), entry_2.get()),\n",
    "        relief=\"flat\"\n",
    "    )\n",
    "    button_1.place(\n",
    "        x=282.0,\n",
    "        y=401.0,\n",
    "        width=68.0,\n",
    "        height=40.0\n",
    "    )\n",
    "    \n",
    "    window.resizable(False, False)\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5874444b-6063-4bec-80d7-7f6cde56c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\torch-shit\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "view_main_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297028f9-cead-4df3-b28e-bf03f2973a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037835305ba744119d025f5cf89e5b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04dfd6992af846d5abb51cd1266db7cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05af80238f6f41249b7842dccc0b82cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "074bab26bb3c43029575c4fc67f5137d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07e729be45284f28a3ee15be408e2d3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bf308066b194d43b6de1b53b284ab91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ef238c33f9842da9da7017d4571f114": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56beed392210489c801831cedec0bc6d",
       "IPY_MODEL_7ec0fb5dffa94d4fa3f0342cbf91e7c9",
       "IPY_MODEL_3cb8ccebe7254d0fba7c0d954b42192f"
      ],
      "layout": "IPY_MODEL_3ac4592c4bc641a3aa2025e3c471adeb"
     }
    },
    "10734bb2699045aca2be866650738c72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2630088666f34f5db5e02a0afd369b29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26db42ee704f4c5a87f23a638b6f991e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2840ed9160c343dbb54d918c3be64624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9910f922d9364c349b736726b17d64f4",
      "placeholder": "​",
      "style": "IPY_MODEL_7bdab68e13034718b24f952d35e8790a",
      "value": " 825k/825k [00:00&lt;00:00, 8.00MB/s]"
     }
    },
    "2b7055cd8c814162b1b39c4e885c6bb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feaae86c965a4948b853bd7e3ddf7f40",
      "max": 543432324,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f6449ca3b95470b85b07d56acecda34",
      "value": 543432324
     }
    },
    "2cea7303a0a3423d98308f7df3dbb2d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3337553386ce4938a7ac2e89edb1ae90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33e4c43a798b4af09d7931b180fac455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7889cc182874533a9e574cc701ba331",
      "placeholder": "​",
      "style": "IPY_MODEL_4215fa3b2ed94bdc83fc93803326ede3",
      "value": " 543M/543M [00:05&lt;00:00, 169MB/s]"
     }
    },
    "3ac4592c4bc641a3aa2025e3c471adeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cb8ccebe7254d0fba7c0d954b42192f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_573f6a9dbee241d59ce1b8561553d328",
      "placeholder": "​",
      "style": "IPY_MODEL_037835305ba744119d025f5cf89e5b0d",
      "value": " 384/384 [00:00&lt;00:00, 16.4kB/s]"
     }
    },
    "4215fa3b2ed94bdc83fc93803326ede3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "424c23a3930d482cab25093e195ab319": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53caf4b693084572ad9cb0cce751c1ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56beed392210489c801831cedec0bc6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6f27137b2264ecabc7ed6334fb9bce6",
      "placeholder": "​",
      "style": "IPY_MODEL_10734bb2699045aca2be866650738c72",
      "value": "config.json: 100%"
     }
    },
    "573f6a9dbee241d59ce1b8561553d328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c452371f95f4adca3b64bd191aa1f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf5e0696b2bd4740a7e7c3e97f807a40",
      "placeholder": "​",
      "style": "IPY_MODEL_99b1778434a34fbc981f6fb384012797",
      "value": "tokenizer.json: 100%"
     }
    },
    "5cde65fe85a448b0a86cbf8ea0c2a7ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5def9acd41a341ebaa9baaca5bfe81a6",
      "max": 824793,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c3547f0e6215473e99c9bffac63d57fb",
      "value": 824793
     }
    },
    "5def9acd41a341ebaa9baaca5bfe81a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6581bb798dbf4d6d8486e6d8c5944f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9f6c51cbf5f403fa9107e0a0d63bf53",
       "IPY_MODEL_a0bdb1425e3c44db8514a18d2eea4663",
       "IPY_MODEL_c15c6717b560495fbf3b958a32fe3e87"
      ],
      "layout": "IPY_MODEL_bdfdba7e7a174bf8b1b51b6041fcb7d5"
     }
    },
    "6f6449ca3b95470b85b07d56acecda34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7794613e7c2547059737fee6406e33f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a8aab4bbe7c4c55b24f53b375c1576b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bdab68e13034718b24f952d35e8790a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec0fb5dffa94d4fa3f0342cbf91e7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e858ef86c45943e981f05b42622a24fa",
      "max": 384,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3337553386ce4938a7ac2e89edb1ae90",
      "value": 384
     }
    },
    "8d79d6bc8ae7456aab50ae7bc8b8f461": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e95365fdbe044eda8aa4735cfefe007": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "952cde71642f4a39bc19090f340ecd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef27dcd62c2e4d7da22177a1f85eb992",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af0c86996560491f96e664a3ce0e2f48",
      "value": 112
     }
    },
    "9910f922d9364c349b736726b17d64f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99b1778434a34fbc981f6fb384012797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cf9c13bc79e4fc382c354fe2a72d416": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cea7303a0a3423d98308f7df3dbb2d9",
      "max": 2642362,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d30dc661d42a4794a5e4f06d84bd71a0",
      "value": 2642362
     }
    },
    "a0bdb1425e3c44db8514a18d2eea4663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2630088666f34f5db5e02a0afd369b29",
      "max": 381,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53caf4b693084572ad9cb0cce751c1ca",
      "value": 381
     }
    },
    "a58d82a0ef144d0baea39f0fd3d22365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7892941140f40cdb6a0d08d05e71dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c28c42f5e5b242aab645486d5db2623c",
      "placeholder": "​",
      "style": "IPY_MODEL_8e95365fdbe044eda8aa4735cfefe007",
      "value": "vocab.txt: 100%"
     }
    },
    "a9f6c51cbf5f403fa9107e0a0d63bf53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_caf310f8e5dc436d9cd18ad9e6dda77a",
      "placeholder": "​",
      "style": "IPY_MODEL_cfd58728c1c64242879e3cd0d424dcf7",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "abb21c1d50c44e49a03464a966922b2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c452371f95f4adca3b64bd191aa1f02",
       "IPY_MODEL_9cf9c13bc79e4fc382c354fe2a72d416",
       "IPY_MODEL_f4f5040f35f74cf08b64ba5f4b2ed075"
      ],
      "layout": "IPY_MODEL_7794613e7c2547059737fee6406e33f9"
     }
    },
    "af0c86996560491f96e664a3ce0e2f48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7f1f951abe1454ea84d0c7703b68def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07e729be45284f28a3ee15be408e2d3e",
      "placeholder": "​",
      "style": "IPY_MODEL_bc01d36956484be6abc2be865579de02",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "b7fa0905c26343a6979a264fb53dacb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7892941140f40cdb6a0d08d05e71dda",
       "IPY_MODEL_5cde65fe85a448b0a86cbf8ea0c2a7ef",
       "IPY_MODEL_2840ed9160c343dbb54d918c3be64624"
      ],
      "layout": "IPY_MODEL_26db42ee704f4c5a87f23a638b6f991e"
     }
    },
    "bc01d36956484be6abc2be865579de02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdfdba7e7a174bf8b1b51b6041fcb7d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c15c6717b560495fbf3b958a32fe3e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_424c23a3930d482cab25093e195ab319",
      "placeholder": "​",
      "style": "IPY_MODEL_a58d82a0ef144d0baea39f0fd3d22365",
      "value": " 381/381 [00:00&lt;00:00, 18.0kB/s]"
     }
    },
    "c28c42f5e5b242aab645486d5db2623c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3547f0e6215473e99c9bffac63d57fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3d8ef1aae4b447899332e3f5eaeb09d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f85778a72149439a899ad89d5f0b3163",
       "IPY_MODEL_2b7055cd8c814162b1b39c4e885c6bb0",
       "IPY_MODEL_33e4c43a798b4af09d7931b180fac455"
      ],
      "layout": "IPY_MODEL_04dfd6992af846d5abb51cd1266db7cc"
     }
    },
    "c6f27137b2264ecabc7ed6334fb9bce6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caf310f8e5dc436d9cd18ad9e6dda77a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cee92b770bfb4181ae84ef88a4621b60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf5e0696b2bd4740a7e7c3e97f807a40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfd58728c1c64242879e3cd0d424dcf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d30dc661d42a4794a5e4f06d84bd71a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5359ee0b045420bb6202139f79c1445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7f1f951abe1454ea84d0c7703b68def",
       "IPY_MODEL_952cde71642f4a39bc19090f340ecd56",
       "IPY_MODEL_f2f9d499e4f54303bbb031705c181635"
      ],
      "layout": "IPY_MODEL_cee92b770bfb4181ae84ef88a4621b60"
     }
    },
    "dff708c3c1db4a40abe1602778e4b839": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7889cc182874533a9e574cc701ba331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e858ef86c45943e981f05b42622a24fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef27dcd62c2e4d7da22177a1f85eb992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2f9d499e4f54303bbb031705c181635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dff708c3c1db4a40abe1602778e4b839",
      "placeholder": "​",
      "style": "IPY_MODEL_8d79d6bc8ae7456aab50ae7bc8b8f461",
      "value": " 112/112 [00:00&lt;00:00, 6.69kB/s]"
     }
    },
    "f4f5040f35f74cf08b64ba5f4b2ed075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bf308066b194d43b6de1b53b284ab91",
      "placeholder": "​",
      "style": "IPY_MODEL_074bab26bb3c43029575c4fc67f5137d",
      "value": " 2.64M/2.64M [00:00&lt;00:00, 25.1MB/s]"
     }
    },
    "f85778a72149439a899ad89d5f0b3163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05af80238f6f41249b7842dccc0b82cc",
      "placeholder": "​",
      "style": "IPY_MODEL_7a8aab4bbe7c4c55b24f53b375c1576b",
      "value": "model.safetensors: 100%"
     }
    },
    "feaae86c965a4948b853bd7e3ddf7f40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
